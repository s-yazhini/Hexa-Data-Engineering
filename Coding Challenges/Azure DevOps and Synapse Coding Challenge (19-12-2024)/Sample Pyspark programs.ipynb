{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6981558-7518-4c32-b06f-02b35e31ea2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.core.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#initialize the program\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)])\n",
    "print(type(rdd))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b40441-08ac-4c27-86aa-900b073f42d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\nParallelCollectionRDD[0] at readRDDFromInputStream at PythonRDD.scala:450\n+--------+-------+-----------+-------+---------+\n|Division|English|Mathematics|Physics|Chemistry|\n+--------+-------+-----------+-------+---------+\n|       C|     85|         76|     87|       91|\n|       B|     85|         76|     87|       91|\n|       A|     85|         78|     96|       92|\n|       A|     92|         76|     89|       96|\n+--------+-------+-----------+-------+---------+\n\nroot\n |-- Division: string (nullable = true)\n |-- English: long (nullable = true)\n |-- Mathematics: long (nullable = true)\n |-- Physics: long (nullable = true)\n |-- Chemistry: long (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('C', 85, 76, 87, 91),\n",
       " ('B', 85, 76, 87, 91),\n",
       " ('A', 85, 78, 96, 92),\n",
       " ('A', 92, 76, 89, 96)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "print(type(marks_df))\n",
    "print(rdd)\n",
    "marks_df.show()\n",
    "marks_df.printSchema()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d39e80-3ca9-48f5-a5b8-e01b0dcefe83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \n",
    "                    .appName('pyspark_ex').getOrCreate()\n",
    "\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddec58c-8f73-46f8-98cf-3f93b3b57fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+-----------+\n|cust_id|cust_fname|cust_lname|cust_order|cust_status|\n+-------+----------+----------+----------+-----------+\n|      1|      john|       doe|         5|     active|\n|      2|      jane|     smith|         8|     active|\n|      3|   micheal|   jhonson|         3|   inactive|\n|      4|      abhi|   wiliams|         1|     active|\n|      5|       ram|     brown|         4|   inactive|\n|      6|     emily|  anderson|         2|     active|\n|      7|   william|     jones|        10|     active|\n|      8|     susan|     davis|         7|   inactive|\n|      9|     david|    miller|         9|     active|\n|     10|      sara|     moore|         2|   inactive|\n|     11|     james|    tailor|         5|   inactive|\n|     12|    olivia|    wilson|         3|   inactive|\n|     13|    robert|     evans|        11|     active|\n|     14|      emma|    thomas|        29|     active|\n|     15|    mathew|     haris|         5|   inactive|\n|     16|  isabella|     white|         6|   inactive|\n|     17|    joseph|    martin|         4|   inactive|\n|     18|     grace|       lee|         5|     active|\n|     19|chrisopher|      basa|         8|   inactive|\n|     20|       ava|    joesph|         3|     active|\n+-------+----------+----------+----------+-----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cust_id</th><th>cust_fname</th><th>cust_lname</th><th>cust_order</th><th>cust_status</th></tr></thead><tbody><tr><td>1</td><td>john</td><td>doe</td><td>5</td><td>active</td></tr><tr><td>2</td><td>jane</td><td>smith</td><td>8</td><td>active</td></tr><tr><td>3</td><td>micheal</td><td>jhonson</td><td>3</td><td>inactive</td></tr><tr><td>4</td><td>abhi</td><td>wiliams</td><td>1</td><td>active</td></tr><tr><td>5</td><td>ram</td><td>brown</td><td>4</td><td>inactive</td></tr><tr><td>6</td><td>emily</td><td>anderson</td><td>2</td><td>active</td></tr><tr><td>7</td><td>william</td><td>jones</td><td>10</td><td>active</td></tr><tr><td>8</td><td>susan</td><td>davis</td><td>7</td><td>inactive</td></tr><tr><td>9</td><td>david</td><td>miller</td><td>9</td><td>active</td></tr><tr><td>10</td><td>sara</td><td>moore</td><td>2</td><td>inactive</td></tr><tr><td>11</td><td>james</td><td>tailor</td><td>5</td><td>inactive</td></tr><tr><td>12</td><td>olivia</td><td>wilson</td><td>3</td><td>inactive</td></tr><tr><td>13</td><td>robert</td><td>evans</td><td>11</td><td>active</td></tr><tr><td>14</td><td>emma</td><td>thomas</td><td>29</td><td>active</td></tr><tr><td>15</td><td>mathew</td><td>haris</td><td>5</td><td>inactive</td></tr><tr><td>16</td><td>isabella</td><td>white</td><td>6</td><td>inactive</td></tr><tr><td>17</td><td>joseph</td><td>martin</td><td>4</td><td>inactive</td></tr><tr><td>18</td><td>grace</td><td>lee</td><td>5</td><td>active</td></tr><tr><td>19</td><td>chrisopher</td><td>basa</td><td>8</td><td>inactive</td></tr><tr><td>20</td><td>ava</td><td>joesph</td><td>3</td><td>active</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "john",
         "doe",
         5,
         "active"
        ],
        [
         2,
         "jane",
         "smith",
         8,
         "active"
        ],
        [
         3,
         "micheal",
         "jhonson",
         3,
         "inactive"
        ],
        [
         4,
         "abhi",
         "wiliams",
         1,
         "active"
        ],
        [
         5,
         "ram",
         "brown",
         4,
         "inactive"
        ],
        [
         6,
         "emily",
         "anderson",
         2,
         "active"
        ],
        [
         7,
         "william",
         "jones",
         10,
         "active"
        ],
        [
         8,
         "susan",
         "davis",
         7,
         "inactive"
        ],
        [
         9,
         "david",
         "miller",
         9,
         "active"
        ],
        [
         10,
         "sara",
         "moore",
         2,
         "inactive"
        ],
        [
         11,
         "james",
         "tailor",
         5,
         "inactive"
        ],
        [
         12,
         "olivia",
         "wilson",
         3,
         "inactive"
        ],
        [
         13,
         "robert",
         "evans",
         11,
         "active"
        ],
        [
         14,
         "emma",
         "thomas",
         29,
         "active"
        ],
        [
         15,
         "mathew",
         "haris",
         5,
         "inactive"
        ],
        [
         16,
         "isabella",
         "white",
         6,
         "inactive"
        ],
        [
         17,
         "joseph",
         "martin",
         4,
         "inactive"
        ],
        [
         18,
         "grace",
         "lee",
         5,
         "active"
        ],
        [
         19,
         "chrisopher",
         "basa",
         8,
         "inactive"
        ],
        [
         20,
         "ava",
         "joesph",
         3,
         "active"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cust_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "cust_fname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cust_lname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cust_order",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "cust_status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "data =spark.read.csv(\"/FileStore/tables/orders.csv\",header = True,inferSchema = True)\n",
    "data.show()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5250243b-7417-4136-b494-6e667f4a7dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+-----------+\n|    _c0|       _c1|       _c2|       _c3|        _c4|\n+-------+----------+----------+----------+-----------+\n|cust_id|cust_fname|cust_lname|cust_order|cust_status|\n|      1|      john|       doe|         5|     active|\n|      2|      jane|     smith|         8|     active|\n|      3|   micheal|   jhonson|         3|   inactive|\n|      4|      abhi|   wiliams|         1|     active|\n|      5|       ram|     brown|         4|   inactive|\n|      6|     emily|  anderson|         2|     active|\n|      7|   william|     jones|        10|     active|\n|      8|     susan|     davis|         7|   inactive|\n|      9|     david|    miller|         9|     active|\n|     10|      sara|     moore|         2|   inactive|\n|     11|     james|    tailor|         5|   inactive|\n|     12|    olivia|    wilson|         3|   inactive|\n|     13|    robert|     evans|        11|     active|\n|     14|      emma|    thomas|        29|     active|\n|     15|    mathew|     haris|         5|   inactive|\n|     16|  isabella|     white|         6|   inactive|\n|     17|    joseph|    martin|         4|   inactive|\n|     18|     grace|       lee|         5|     active|\n|     19|chrisopher|      basa|         8|   inactive|\n+-------+----------+----------+----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th></tr></thead><tbody><tr><td>cust_id</td><td>cust_fname</td><td>cust_lname</td><td>cust_order</td><td>cust_status</td></tr><tr><td>1</td><td>john</td><td>doe</td><td>5</td><td>active</td></tr><tr><td>2</td><td>jane</td><td>smith</td><td>8</td><td>active</td></tr><tr><td>3</td><td>micheal</td><td>jhonson</td><td>3</td><td>inactive</td></tr><tr><td>4</td><td>abhi</td><td>wiliams</td><td>1</td><td>active</td></tr><tr><td>5</td><td>ram</td><td>brown</td><td>4</td><td>inactive</td></tr><tr><td>6</td><td>emily</td><td>anderson</td><td>2</td><td>active</td></tr><tr><td>7</td><td>william</td><td>jones</td><td>10</td><td>active</td></tr><tr><td>8</td><td>susan</td><td>davis</td><td>7</td><td>inactive</td></tr><tr><td>9</td><td>david</td><td>miller</td><td>9</td><td>active</td></tr><tr><td>10</td><td>sara</td><td>moore</td><td>2</td><td>inactive</td></tr><tr><td>11</td><td>james</td><td>tailor</td><td>5</td><td>inactive</td></tr><tr><td>12</td><td>olivia</td><td>wilson</td><td>3</td><td>inactive</td></tr><tr><td>13</td><td>robert</td><td>evans</td><td>11</td><td>active</td></tr><tr><td>14</td><td>emma</td><td>thomas</td><td>29</td><td>active</td></tr><tr><td>15</td><td>mathew</td><td>haris</td><td>5</td><td>inactive</td></tr><tr><td>16</td><td>isabella</td><td>white</td><td>6</td><td>inactive</td></tr><tr><td>17</td><td>joseph</td><td>martin</td><td>4</td><td>inactive</td></tr><tr><td>18</td><td>grace</td><td>lee</td><td>5</td><td>active</td></tr><tr><td>19</td><td>chrisopher</td><td>basa</td><td>8</td><td>inactive</td></tr><tr><td>20</td><td>ava</td><td>joesph</td><td>3</td><td>active</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "cust_id",
         "cust_fname",
         "cust_lname",
         "cust_order",
         "cust_status"
        ],
        [
         "1",
         "john",
         "doe",
         "5",
         "active"
        ],
        [
         "2",
         "jane",
         "smith",
         "8",
         "active"
        ],
        [
         "3",
         "micheal",
         "jhonson",
         "3",
         "inactive"
        ],
        [
         "4",
         "abhi",
         "wiliams",
         "1",
         "active"
        ],
        [
         "5",
         "ram",
         "brown",
         "4",
         "inactive"
        ],
        [
         "6",
         "emily",
         "anderson",
         "2",
         "active"
        ],
        [
         "7",
         "william",
         "jones",
         "10",
         "active"
        ],
        [
         "8",
         "susan",
         "davis",
         "7",
         "inactive"
        ],
        [
         "9",
         "david",
         "miller",
         "9",
         "active"
        ],
        [
         "10",
         "sara",
         "moore",
         "2",
         "inactive"
        ],
        [
         "11",
         "james",
         "tailor",
         "5",
         "inactive"
        ],
        [
         "12",
         "olivia",
         "wilson",
         "3",
         "inactive"
        ],
        [
         "13",
         "robert",
         "evans",
         "11",
         "active"
        ],
        [
         "14",
         "emma",
         "thomas",
         "29",
         "active"
        ],
        [
         "15",
         "mathew",
         "haris",
         "5",
         "inactive"
        ],
        [
         "16",
         "isabella",
         "white",
         "6",
         "inactive"
        ],
        [
         "17",
         "joseph",
         "martin",
         "4",
         "inactive"
        ],
        [
         "18",
         "grace",
         "lee",
         "5",
         "active"
        ],
        [
         "19",
         "chrisopher",
         "basa",
         "8",
         "inactive"
        ],
        [
         "20",
         "ava",
         "joesph",
         "3",
         "active"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c4",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "data =spark.read.csv(\"/FileStore/tables/orders.csv\")\n",
    "data.show()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163c37ee-d878-48ee-a514-b4023a3b786f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n+---------+--------+------+------+----------+\n|firstname|lastname|gender|salary|new_column|\n+---------+--------+------+------+----------+\n|    James|   Smith|     M|  3000|         1|\n|     Anna|    Rose|     F|  4100|         1|\n|   Robert|Williams|     M|  6200|         1|\n+---------+--------+------+------+----------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|other_column|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|       30000|\n|     Anna|    Rose|     F|  4100|       41000|\n|   Robert|Williams|     M|  6200|       62000|\n+---------+--------+------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('pyspark_ex').getOrCreate()\n",
    "\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "#1.write a program for adding a new column\n",
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"new_column\",lit(1)).show()\n",
    "df.withColumn(\"other_column\",df.salary*10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fae6e8c-e261-4852-a6a3-b97d8321837f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+----------+------+\n|  name| id|age|department|salary|\n+------+---+---+----------+------+\n| user1|  1| 25|Jr manager| 98000|\n| user2|  2| 30|sr manager|100000|\n| user3|  6| 35|sr manager|100000|\n| user4|  4| 32|      head| 70000|\n| user5|  1| 45|Jr manager| 60000|\n| user6|  6| 47|     head2| 45000|\n| user7|  5| 21|    worker| 25000|\n| user8|  1| 22|Jr manager| 50000|\n| user9| 10| 54|      lead| 45000|\n|user10| 59| 52|     lead2| 50000|\n|user11|  6| 25|     head2| 50000|\n|user12|  2| 27|sr manager| 70000|\n|user13| 59| 54|     lead2| 45000|\n|user14|  2| 25|sr manager| 70000|\n|user15|  1| 32|Jr manager| 50000|\n|user16|  3| 37|    worker| 25000|\n|user17| 74| 63|   Manager| 68000|\n|user18|  7| 25|      head| 45000|\n|user19| 10| 32| lvl2 head| 52000|\n|user20| 10| 32| lvl2 head| 52000|\n+------+---+---+----------+------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>id</th><th>age</th><th>department</th><th>salary</th></tr></thead><tbody><tr><td>user1</td><td>1</td><td>25</td><td>Jr manager</td><td>98000</td></tr><tr><td>user2</td><td>2</td><td>30</td><td>sr manager</td><td>100000</td></tr><tr><td>user3</td><td>6</td><td>35</td><td>sr manager</td><td>100000</td></tr><tr><td>user4</td><td>4</td><td>32</td><td>head</td><td>70000</td></tr><tr><td>user5</td><td>1</td><td>45</td><td>Jr manager</td><td>60000</td></tr><tr><td>user6</td><td>6</td><td>47</td><td>head2</td><td>45000</td></tr><tr><td>user7</td><td>5</td><td>21</td><td>worker</td><td>25000</td></tr><tr><td>user8</td><td>1</td><td>22</td><td>Jr manager</td><td>50000</td></tr><tr><td>user9</td><td>10</td><td>54</td><td>lead</td><td>45000</td></tr><tr><td>user10</td><td>59</td><td>52</td><td>lead2</td><td>50000</td></tr><tr><td>user11</td><td>6</td><td>25</td><td>head2</td><td>50000</td></tr><tr><td>user12</td><td>2</td><td>27</td><td>sr manager</td><td>70000</td></tr><tr><td>user13</td><td>59</td><td>54</td><td>lead2</td><td>45000</td></tr><tr><td>user14</td><td>2</td><td>25</td><td>sr manager</td><td>70000</td></tr><tr><td>user15</td><td>1</td><td>32</td><td>Jr manager</td><td>50000</td></tr><tr><td>user16</td><td>3</td><td>37</td><td>worker</td><td>25000</td></tr><tr><td>user17</td><td>74</td><td>63</td><td>Manager</td><td>68000</td></tr><tr><td>user18</td><td>7</td><td>25</td><td>head</td><td>45000</td></tr><tr><td>user19</td><td>10</td><td>32</td><td>lvl2 head</td><td>52000</td></tr><tr><td>user20</td><td>10</td><td>32</td><td>lvl2 head</td><td>52000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "user1",
         1,
         25,
         "Jr manager",
         98000
        ],
        [
         "user2",
         2,
         30,
         "sr manager",
         100000
        ],
        [
         "user3",
         6,
         35,
         "sr manager",
         100000
        ],
        [
         "user4",
         4,
         32,
         "head",
         70000
        ],
        [
         "user5",
         1,
         45,
         "Jr manager",
         60000
        ],
        [
         "user6",
         6,
         47,
         "head2",
         45000
        ],
        [
         "user7",
         5,
         21,
         "worker",
         25000
        ],
        [
         "user8",
         1,
         22,
         "Jr manager",
         50000
        ],
        [
         "user9",
         10,
         54,
         "lead",
         45000
        ],
        [
         "user10",
         59,
         52,
         "lead2",
         50000
        ],
        [
         "user11",
         6,
         25,
         "head2",
         50000
        ],
        [
         "user12",
         2,
         27,
         "sr manager",
         70000
        ],
        [
         "user13",
         59,
         54,
         "lead2",
         45000
        ],
        [
         "user14",
         2,
         25,
         "sr manager",
         70000
        ],
        [
         "user15",
         1,
         32,
         "Jr manager",
         50000
        ],
        [
         "user16",
         3,
         37,
         "worker",
         25000
        ],
        [
         "user17",
         74,
         63,
         "Manager",
         68000
        ],
        [
         "user18",
         7,
         25,
         "head",
         45000
        ],
        [
         "user19",
         10,
         32,
         "lvl2 head",
         52000
        ],
        [
         "user20",
         10,
         32,
         "lvl2 head",
         52000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "data =spark.read.csv(\"/FileStore/tables/salary-2.csv\",header = True,inferSchema = True)\n",
    "\n",
    "data.limit(10).toPandas\n",
    "data.show()\n",
    "display(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ff9e9f-edf2-4e55-959f-bf51326fa916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name  Age\n0   Scott   50\n1    Jeff   45\n2  Thomas   54\n3     Ann   34\n+------+---+\n|  Name|Age|\n+------+---+\n| Scott| 50|\n|  Jeff| 45|\n|Thomas| 54|\n|   Ann| 34|\n+------+---+\n\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Converting Pandasdf to spark df\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "sparkdf =spark.createDataFrame(pandasDF)\n",
    "sparkdf.show()\n",
    "sparkdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7378c01e-9270-4b54-bdd3-44ecb5c9fb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- First Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b42326-f2d8-4229-aed5-291e0f332365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2d0003-119d-4a6b-8309-a1026303b28c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark_ex4').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
    "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
    "    .select(col(\"date\"),col(\"increment\"), \\\n",
    "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72b9437-90ea-4367-b9b6-256dfff7fab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Pyspark programs",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
